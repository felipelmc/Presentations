---
title: "Automated Text Analysis"
subtitle: "Summer Institute in Computational Social Science (SICSS) 2025"
author: "Felipe Lamarca"
institute: "Instituto de Estudos Sociais e Pol√≠ticos (IESP-UERJ)"
date: "2025-07-02"
format: 
  beamer:
    theme: "Rochester"       # exemplo de tema
    colortheme: "dolphin"      # tema de cores
    fonttheme: "professionalfonts"
linkcolor: blue
fontsize: 10pt
---


# Text as data

Nas ci√™ncias sociais (e em v√°rias outras ci√™ncias), textos s√£o fontes de **evid√™ncia emp√≠rica**:

- Discursos de pol√≠ticos
- Publica√ß√µes em redes sociais
- Respostas a perguntas abertas em surveys
- Documentos hist√≥ricos
- ...

Intuitivamente, a ideia do "texto como dado" √© natural, para n√£o dizer √≥bvia.


# Text as data

Podemos analisar textos de v√°rias formas. Nas ci√™ncias sociais, em particular, a an√°lise de texto por muito texto esteve restrita a m√©todos qualitativos: an√°lise de conte√∫do, de discurso e assim por diante.

A an√°lise quantitativa de texto √© uma oportunidade de analisar corpus textuais **maiores** e de forma mais [sistem√°tica]{.underline}.

# A evolu√ß√£o do text as data

At√© recentemente, a an√°lise quantitativa de texto era baseada em m√©todos como:

- Bag of Words
- Contagem de palavras
- M√©todos baseados em dicion√°rio
- Modelagem de t√≥pico
- N-gramas

S√£o, √© claro, m√©todos muito √∫teis. No entanto, eles s√£o incapazes de incorporar **contexto** e **significado** de frases e palavras.

# Embeddings: textos $\rightarrow$ vetores

O que diferencia um banco de um banco?

:::: {.columns}

::: {.column width="50%"}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/banco_de_dinheiro.jpg}
    \caption{Banco}
\end{figure}
:::

::: {.column width="50%"}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/banco_de_sentar.jpg}
    \caption{Banco}
\end{figure}
:::

::::


# Embeddings: textos $\rightarrow$ vetores

Modelos de linguagem precisam representar palavras e frases como vetores num√©ricos para process√°-las. Esses vetores s√£o chamados de **embeddings**.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/embeddings3b1b.png}
    \caption{Word Embeddings -- Fonte: 3b1b}
\end{figure}

# Embeddings: textos representados como vetores num√©ricos

A ideia √© que, ao treinarmos um modelo de linguagem, a representa√ß√£o vetorial de palavras e frases passa a capturar algum significado sem√¢ntico.

[Embedding Projector](https://projector.tensorflow.org/)

# Transformer: o que √©?

:::: {.columns}

::: {.column width="60%"}
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/transformer_architecture.png}
    \caption{Arquitetura de um Transformer}
\end{figure}
:::

::: {.column width="40%"}
_Transformers_ s√£o uma arquitetura de rede neural que revolucionou o campo de NLP ao permitir que os modelos aprendam rela√ß√µes contextuais entre palavras em uma frase a partir do mecanismo de _attention_. [[link]](https://arxiv.org/abs/1706.03762)
:::

::::

# Transformer: o que faz?

Na pr√°tica, os _transformers_ s√£o capazes de prever a pr√≥xima palavra em uma frase, levando em considera√ß√£o o contexto de todas as palavras anteriores.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/transformer3b1b.png}
    \caption{Transformer -- Fonte: 3b1b}
\end{figure}

# Fine-tuning: como adaptar modelos para tarefas espec√≠ficas

A t√©cnica de fine-tuning envolve ajustar um modelo pr√©-treinado em um conjunto de dados espec√≠fico para melhorar seu desempenho em uma tarefa particular. Isso √© especialmente √∫til quando se tem um conjunto de dados limitado, mas ainda assim se deseja aproveitar o conhecimento adquirido pelo modelo durante o treinamento inicial.

# Modelos "grandes" e modelos "pequenos"

Em geral, modelos de linguagem podem ser "grandes" ou "pequenos": 

- **Modelos grandes:** treinados com grandes quantidades de dados, geralmente em v√°rias l√≠nguas, e com muitos par√¢metros. Exemplo: `llama3.1:405b`.
- **Modelos pequenos:** treinados com menos dados e com menos par√¢metros. Menor capacidade de generaliza√ß√£o. Exemplo: `llama3.2:1b`.

### Saiba {.alert}

Em geral, quanto maior o modelo, melhor o desempenho; no entanto, menores s√£o as chances de que voc√™ consiga rodar o modelo no seu pr√≥prio computador.

# No que usar?

O c√©u √© o limite! Alguns exemplos:

- _Tradu√ß√£o_ de textos
- _Transcri√ß√£o_ de textos
- _Classifica√ß√£o_ de textos
- _C√°lculo de dist√¢ncia_ (i.e., similaridade) entre textos
- Reconhecimento de entidades nomeadas

# Como usar?

Beleza, Felipe, j√° entendi. Mas como eu uso esse neg√≥cio?

Op√ß√µes open-source

- HuggingFace
- Ollama
- Meta

Op√ß√µes pagas

- Modelos open-source via Groq (com limite gratuito di√°rio)
- ChatGPT, da OpenAI
- Grok, da xAI
- Claude, da Anthropic
- ...

# Indica√ß√µes de material

[[link]](https://arxiv.org/pdf/2305.03514) Can Large Language Models Transform Computational Social Science? (2024)

[[link]](https://www.scielo.br/j/rsocp/a/W34dV4fXQfthZmSmQLmkXzs/?format=pdf&lang=pt) Do 'texto como texto' ao 'texto como dado': o potencial das pesquisas em Rela√ß√µes Internacionais (2022)

[[link]](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) S√©rie de v√≠deos do 3Blue1Brown sobre redes neurais e _transformers_.

[[link]](https://epoch.ai/blog/how-much-does-it-cost-to-train-frontier-ai-models) How Much Does It Cost to Train Frontier AI Models? (2024)

# Contato

**‚úâÔ∏è Email:** [felipe.lamarca@hotmail.com](mailto:felipe.lamarca@hotmail.com)  
**üåê Site:** [felipelamarca.com](https://felipelamarca.com)  
**üîó LinkedIn:** [felipe-lamarca](https://www.linkedin.com/in/felipe-lamarca)
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dXfhbzUdNjL"
      },
      "source": [
        "# **Workshop**: Automated Text Analysis\n",
        "\n",
        "## **Instructor:** [Felipe Lamarca (IESP-UERJ)](https://felipelamarca.com/)\n",
        "\n",
        "Neste notebook, vamos ver como utilizar alguns modelos disponíveis no HuggingFace. Vamos começar fazendo o download das bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbtLs4jXcd1-"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers transformers datasets huggingface_hub fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKApPQ7i_VEZ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2fuowN3CZva"
      },
      "source": [
        "Vamos fazer o download de um modelo. Em especial, esse é um modelo de linguagem pequeno e pré-treinado com dados em português."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yNxv-hjeLXp"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb3BMYxFCl4J"
      },
      "source": [
        "Agora, vamos computar os embeddings de cada uma das frases abaixo. Lembre-se: embeddings são representações numéricas de palavras ou frases e, graças aos transformers, carregam características semânticas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gLMlZFzEDd5"
      },
      "source": [
        "## **Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUZ4G0ceAeEz"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"O Banco Central aumentou a taxa de juros.\",\n",
        "    \"Sentei-me num banco\",\n",
        "    \"O Flamengo vencerá contra o Bayern.\"\n",
        "]\n",
        "embeddings = model.encode(sentences, normalize_embeddings=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL9ympkjBPsq"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(embeddings)\n",
        "print(np.round(sim, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwnO80GYEH_a"
      },
      "source": [
        "## **Análise de sentimento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEnkxvHiBhWq"
      },
      "outputs": [],
      "source": [
        "sentiment = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"pysentimiento/bertweet-pt-sentiment\",\n",
        "    top_k=None,              # devolve scores para POS / NEG / NEU\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p62vQ9tTEPAw"
      },
      "outputs": [],
      "source": [
        "sentiment(\"Esse celular é ótimo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLwVpEOpD4n0"
      },
      "outputs": [],
      "source": [
        "sentiment(\"A comida que eu comi estava horrível.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1x43avsD9bG"
      },
      "outputs": [],
      "source": [
        "sentiment(\"Não tenho certeza sobre o que estou sentindo...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdlW7OBdEQui"
      },
      "source": [
        "## **Geração automática de resumos**\n",
        "\n",
        "https://huggingface.co/datasets/UFRGS/brwac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGw3vc6NE2Jy"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"recogna-nlp/ptt5-base-summ-cstnews\",   # T5 pré-treinado no BrWaC + fine-tuning CSTNews\n",
        "    max_new_tokens=80, min_new_tokens=15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIC-l6m1EUhr"
      },
      "outputs": [],
      "source": [
        "texto = \"\"\"\n",
        "A Praia de Copacabana já perdeu 10% da faixa de areia nos últimos dez anos. E, se o planeta continuar aquecendo no ritmo atual, o futuro da orla carioca é sombrio. Segundo estudo da Universidade Federal do Rio de Janeiro (UFRJ), praias como Leme, Copacabana, Ipanema e Leblon podem recuar de 70 a 100 metros até 2100, transformando-se em filetes de areia. Os dados fazem parte de uma projeção baseada em modelos climáticos internacionais e foram publicados na revista Natural Hazards. As informações são do jornal O Globo.\n",
        "O levantamento analisou o impacto da elevação do nível do mar em toda a faixa entre o Porto do Rio e o Leblon, incluindo a Baía de Guanabara, a Lagoa Rodrigo de Freitas e áreas vulneráveis como os manguezais de Guapimirim. Em Botafogo, por exemplo, a perda estimada é de até 70 metros. Em Niterói, o estudo avaliou desde a região portuária até a praia e a lagoa de Piratininga.\n",
        "“O nível do mar mais elevado impedirá que a água de grandes chuvas escoe das áreas mais baixas, como a Lagoa e o entorno da Baía de Guanabara”, explicou a pesquisadora Raquel Toste, do Laboratório de Métodos Computacionais (Lamce) da Coppe/UFRJ, que assina o estudo.\n",
        "\"\"\"\n",
        "\n",
        "print(summarizer(texto)[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC0raKZvGMPB"
      },
      "source": [
        "## **Conversação e prompting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl9-leJwGTVS"
      },
      "outputs": [],
      "source": [
        "# Carregando o modelo ajustado para o português\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"rhaymison/phi-3-portuguese-tom-cat-4k-instruct\",\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtRNldvjGLuC"
      },
      "outputs": [],
      "source": [
        "# Escrevendo um prompt simples\n",
        "prompt = (\n",
        "    \"<s><|system|>\\nVocê é um assistente inteligente e gentil.\\n\"\n",
        "    \"<|user|>\\nExplique o que é aprendizado de máquina em termos simples.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "# Gerando resposta\n",
        "resposta = generator(prompt)[0][\"generated_text\"]\n",
        "\n",
        "# Imprimindo apenas a resposta do assistente\n",
        "print(resposta.split(\"<|assistant|>\")[-1].strip())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
